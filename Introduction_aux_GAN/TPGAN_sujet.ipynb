{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wczghxT56FL"
      },
      "source": [
        "# TP GAN\n",
        "## Prérequis\n",
        "\n",
        "Avoir suivi la formation GAN et avoir quelques bases avec PyTorch. Vous pouvez vous mettre sous les yeux le TP CNN.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "L'objectif de ce TP est de coder un GAN pour générer des petits chats trop mignons. Il abordera les notions principales vues pendant la formation GAN : les CNN, convolutions transposées, batchnorm, binary crossentropy, comment entraîner tout ce beau monde... (et peut-être les problèmes de GAN si ça arrive mais pas de panique).\n",
        "\n",
        "La plupart des détails sont déjà implémentés, vous allez surtout être amené à travailler sur l'architecture du générateur et du discriminateur ainsi que de comprendre comment les entraîner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyemBO3ISvqi"
      },
      "source": [
        "## Les imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOPaFwyAIQJl"
      },
      "source": [
        "\n",
        "On va d'abord importer les librairies nécessaires dont les classiques numpy, matplotlib ainsi que Pytorch (quand même....) et certains de ses modules et classes utilisés utilisés souvent (Dataset, nn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvBgYvzTNL8g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import gdown\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4flw12gySJn6"
      },
      "source": [
        "## Téléchargement du dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz7SLVnrIQJp"
      },
      "source": [
        "\n",
        "Importons maintenant le dataset. On va télécharget le dataset de chats depuis ce [drive](https://drive.google.com/uc?id=1F9I7iDmQ_I9Qsrav1UXlD4OiIBVSU5sl) avec la commande gdown (ou à la mano si vous préférez).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLZNgqQ7Obze"
      },
      "outputs": [],
      "source": [
        "# Téléchargement du dataset\n",
        "url = 'https://drive.google.com/uc?id=1F9I7iDmQ_I9Qsrav1UXlD4OiIBVSU5sl'\n",
        "output = 'dataset.tgz'\n",
        "if not os.path.exists(output):\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Dézippage du dataset\n",
        "\n",
        "def unzip(zip_file, dest_dir):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dest_dir)\n",
        "\n",
        "unzip('dataset.tgz', './')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqylv3scUY76"
      },
      "source": [
        "## Quelques paramètres généraux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9kEINnMIQJq"
      },
      "source": [
        "Les paramètres peuvent être modifiés pour tester un peu (sauf peut-être la taille de l'image pour ce dataset).\n",
        "\n",
        "Pour ce dataset, on peut prendre une batch size pas trop grande sinon Colab crash (ou votre PC). De même, la dimension de l'espace latent peut être ajustée, ici vu que les chats c'est pas si simple, environ 100 c'est bien.\n",
        "\n",
        "Pour charger le dataset, on parcourt notre dossier dataset en chargeant les images qui s'y trouvent. Les images ont leurs pixels entre 0 et 255 qu'on va renormaliser dès la création du dataset entre -1 et 1, ce qui est plus adapté pour les réseaux de neurones et bien pour des GAN car on a une moyenne nulle.\n",
        "\n",
        "On va aussi les afficher parce que c'est bien de savoir sur quoi on travaille quand même (et puis pourquoi se priver de photos de chats...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFtB5YCWO2Pm"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "LATENT_DIM = 100\n",
        "IMG_SHAPE = (64,64,3)\n",
        "\n",
        "x_train = []\n",
        "batch = []\n",
        "i = 0\n",
        "\n",
        "for files in os.listdir(\"./dataset\"):\n",
        "  image = read_image(f\"./dataset/{files}\")\n",
        "\n",
        "  # On normalise les données entre -1 et 1\n",
        "\n",
        "  image = (image - 127.5)/127.5\n",
        "\n",
        "  if i < BATCH_SIZE:\n",
        "    batch.append(image)\n",
        "    i += 1\n",
        "  else:\n",
        "    x_train.append(torch.stack(batch))\n",
        "    batch = []\n",
        "    i = 0\n",
        "\n",
        "\n",
        "x_train = torch.stack(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF6nSfKQP-IM"
      },
      "outputs": [],
      "source": [
        "test_batch = next(iter(x_train))\n",
        "test_batch = test_batch.permute(0,2,3,1)\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "for i in range(25):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.axis('off')\n",
        "  # On oublie pas de faire image * 0.5 + 0.5 pour revenir dans [0,1]\n",
        "  plt.imshow(test_batch[i]*0.5+0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWFiDuRM6ocP"
      },
      "source": [
        "## Pytorch et les GPUs...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijvn1AuX0kqG"
      },
      "source": [
        "\n",
        "Pytorch a besoin qu'on lui précise avec quel appareil on travaille (CPU ou GPU par exemple).\n",
        "\n",
        "Du coup, ici on choisit le meilleur appareil disponible (généralement GPU, sinon CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYHvZDsK7UPY"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZZBKWamPNxO"
      },
      "source": [
        "## Le discriminateur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7ZondMqIQJt"
      },
      "source": [
        "\n",
        "On va d'abord faire le discriminateur.\n",
        "\n",
        "Pour cela on va définir le modèle en utilisant nn.Sequential(), comme dans les autres TP.\n",
        "\n",
        "En entrée, on va avoir une image de dimension (3,64,64) (c'est en couleur pour rappel donc il y a bien 3 canaux).\n",
        "\n",
        "Pour vous aider, un petit rappel des blocs à mettre:\n",
        "- `Conv2D(in_features,out_features,kernel_size,strides,padding)`\n",
        "\n",
        "  Typiquement, on utilise une taille de kernel de 3 et des filtres qui sont des puissances de 2 (32,64,128...).\n",
        "  \n",
        "  Pour rappel, le padding `'same'` équivaut à rajouter des zéros pour garder la même taille d'image en sortie (si les strides sont de 1) et avec `'valid'` on en rajoute pas. `'same'` est donc conseillé ici pour éviter les surprises (et de se torturer la tête sur la taille en sortie même si vous savez évidemment que c'est $\\frac{n+2p-f}{s}+1$). A savoir que sur Pytorch **cela ne marche pas forcément**, et que vous allez peut être devoir la préciser à la main.\n",
        "   \n",
        "   On ne met pas l'activation tout de suite car on ajoute d'abord de la batch normalization pour renormaliser le batch en 0 et ainsi profiter au maximum de la non-linéarité de la ReLU en ce point.\n",
        "\n",
        "  Pour rappel, on augmente le nombre de filtres au fur et à mesure de l'architecture dans le discriminateur.\n",
        "- `BatchNorm2D(in_features)`\n",
        "- `LeakyReLU(alpha=0.2)` (pente de la leaky relu dans le domaine $]-\\infty,0]$)\n",
        "\n",
        "Mettez 3-4 blocs comme ça et faites des essais.\n",
        "\n",
        "Ici pas de `Pooling` car on réduit la taille des images directement avec du stride (typiquement 2 à chaque Convolution en comptant bien la taille qu'on obtient à la fin).\n",
        "\n",
        "En sortie on veut une dimension (1) grâce à une `Flatten` puis des `Dense` en oubliant pas la sigmoïde à la fin.\n",
        "\n",
        "*Alternative* :\n",
        "- C'est aussi possible de bien calculer la taille de l'image pour terminer par une `Convolution` avec une sortie de dimension (1,1,1) puis une `Flatten` (un peu mieux, dit \"Full Convolutional\").\n",
        "\n",
        "  **Exemple** :\n",
        "  - Image de (64,64,3) -> 4 Blocs de convolution avec du stride de 2 : $64/2^4$ -> Feature map de taille (4,4,nombre_de_filtres).\n",
        "\n",
        "    On peut ensuite finir avec une `Conv2D` avec une taille de filtre de 4 et du padding `'valid'` pour juste faire une combinaison de tous les pixels restants, ce qui nous donne bien notre unique pixel de dimension (1,1,1) sans oublier la sigmoïde et on peut ensuite `Flatten` tout ça).\n",
        "\n",
        "\n",
        "Juste ici, petite antisèche de l'architecture si vous séchez. Attention l'écriture est en pseudo-code, il faut l'adapter à Pytorch.\n",
        "<details>\n",
        "<summary>Antisèche</summary>\n",
        "Conv2D(3 filtres en entrée, 32 filtres,kernel taille 3,stride 2,padding 1) -> BN -> LR <br />  \n",
        "-> Conv2D(64,kernel 3,stride 2,padding 1) -> BN -> LR <br />  \n",
        "-> Conv2D(128,3,2,1) -> BN -> LR <br />  \n",
        "-> Conv2D(256,3,2,1) -> BN -> LR <br />  \n",
        "-> Conv2D(1,4,1,1) -> Sigmoïde -> Flatten ou Flatten -> Linear(1) -> Sigmoïde\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TasqSOiOk24X"
      },
      "outputs": [],
      "source": [
        "discriminator = nn.Sequential(\n",
        "            ...\n",
        "            ...\n",
        "            # ICI VOUS AJOUTER LES COUCHES POUR LE DISCRIMINATEUR\n",
        "            ...\n",
        "            ...\n",
        "            nn.Flatten()).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaDZbie7Qw2D"
      },
      "source": [
        "## Le générateur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0X7PhKJIQJu"
      },
      "source": [
        "On va faire le générateur.\n",
        "En entrée, il va prendre un vecteur de l'espace latent de dimension `latent_dim`.\n",
        "\n",
        "On augmente la taille de se vecteur grâce à une couche dense.\n",
        "\n",
        "`Linear(latent_dim, 4x4x1024)`\n",
        "\n",
        "Pour redimensionner notre vecteur en une \"image\" de taille 4 x 4 x 1024, on va pouvoir utiliser la couche `Unflatten()`\n",
        "\n",
        "\n",
        "On peut aussi directement redimensionner le vecteur de l'espace latent sans la couche `Linear`.\n",
        "\n",
        "\n",
        "Pour vous aider, un petit rappel des blocs à mettre ensuite:\n",
        "- `Conv2DTranspose(in_features,out_features,kernel_size,strides,padding,output_padding)`\n",
        "\n",
        "  Pour rappel, on diminue le nombre de filtres au fur et à mesure de l'architecture dans le générateur (512,256,...)\n",
        "\n",
        "- `BatchNorm2D(in_features)`\n",
        "- `ReLU()`\n",
        "\n",
        "Mettez 2-3 blocs comme ça et faites des essais aussi. **Faites bien attention** à la taille de vos images tout au long de l'architecture pour bien avoir la taille d'image finale voulue.\n",
        "\n",
        "**Exemple**:\n",
        "\n",
        "- On a transformé notre vecteur latent en image de dimension (1024,4,4). Pour avoir du (3,64,64), il faut donc 4 blocs de `Conv2DTranspose` (avec padding) pour avoir $4*2^4=64$. Libre à vous de changer l'entrée pour mettre le nombre de blocs que vous voulez.\n",
        "\n",
        "\n",
        "Ici aussi pas de `UpSampling2D` car on augmente la taille avec du stride (2 aussi souvent) à chaque convolution transposée.\n",
        "\n",
        "En sortie on veut une dimension (64,64,3) grâce à une `Conv2D` avec 3 filtres, on oublie pas de prendre une activation en tangente hyperbolique `'tanh'` pour avoir des pixels dans [-1,1].\n",
        "\n",
        "<details>\n",
        "<summary>Antisèche</summary>\n",
        "  (Dans le doute on peut flatten ici)\n",
        "  Linear(100,4*4*1024 neurones) -> Unflatten(en (1024,4,4))\n",
        "  -> ConvTransposée(1024, 256 filtres,kernel taille 3,stride 2,padding 1) -> BN -> ReLU <br />  \n",
        "  -> ConvTransposée(256, 128,kernel 3,stride 2,padding 1) -> BN -> ReLU <br />  \n",
        "  -> ConvTransposée(128, 64,3,2,1) -> BN -> ReLU <br />  \n",
        "  -> ConvTransposée(64, 32,3,2,1) -> BN -> ReLU <br />  \n",
        "  -> ConvTransposée(32, 3,3,1,1) -> Tanh\n",
        "\n",
        "  /!\\ je n'ai pas précisé l'`output_padding` dans l'antisèche, mais **il est peut être nécessaire d'en mettre** !\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHCUgA6lKuwj"
      },
      "outputs": [],
      "source": [
        "generator = nn.Sequential(nn.Linear(LATENT_DIM, 4*4*1024),\n",
        "                          nn.Unflatten(1, (1024, 4, 4)),\n",
        "                          ...\n",
        "                          ...\n",
        "                          # ICI VOUS AJOUTER LES COUCHES POUR LE GENERATEUR\n",
        "                          ...\n",
        "                          ...\n",
        "                          ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Fbk2BnWJOO"
      },
      "source": [
        "## Le train step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrS02DU5IQJw"
      },
      "source": [
        "\n",
        "Cette partie est assez importante car elle permet de comprendre comment on entraîne vraiment un GAN, c'est-à-dire à quoi on compare les sorties pour entraîner correctement le discriminateur et le générateur. Ici, pas de `model.fit` malheureusement.\n",
        "On va définir un `train_step`, c'est à dire ce qu'on va faire comme opérations à chaque batch :\n",
        "\n",
        "Entraîner le discriminateur :\n",
        "\n",
        "- Générer des images fausses à partir de bruit gaussien et en prédire les labels : on a besoin ici d'un vecteur latent de dimension `(batch_size,latent_dim,1,1)`. Ensuite, on fait passer ce bruit dans le générateur pour obtenir des fausses images. Enfin, on récupère la sortie du discriminateur sur celles-ci.\n",
        "\n",
        "\n",
        "```\n",
        "noise = torch.randn(shape).to(device) (merci pytorch...)\n",
        "fake_images = generator(...)\n",
        "fake_predictions = discriminator(...)\n",
        "```\n",
        "\n",
        "\n",
        "- Prendre des images vraies du dataset et en prédire aussi les labels. Donc la sortie du discriminateur sur les vraies images.\n",
        "\n",
        "\n",
        "```\n",
        "real_predictions = ...\n",
        "```\n",
        "\n",
        "\n",
        "- Calculer la loss en comparant les prédictions sur les fausses avec des 0 et les prédictions sur les vraies avec des 1.\n",
        "\n",
        "\n",
        "\n",
        "La binary crossentropy prend en argument les labels visés puis ceux prédits.\n",
        "Pour avoir des 1 ou des 0 : `torch.ones_like(tensor)` ou `torch.zeros_like(tensor)` avec `tensor` le tenseur dont on veut imiter la taille (par exemple les labels issus des prédictions, vu que c'est à ça que l'on va vouloir les comparer). Dans l'exemple suivant, cela corresponds aux `true_labels` et les prédictions précédentes aux `predictions`.\n",
        "```\n",
        "discriminator_loss_on_real = loss(real_labels,real_predictions)\n",
        "discriminator_loss_on_fake = loss(fake_labels,fake_predictions)\n",
        "discriminator_loss = discriminator_loss_on_real + discriminator_loss_on_fake\n",
        "```\n",
        "\n",
        "On peut éventuellement diviser la loss du discriminateur par 2.\n",
        "\n",
        "- Calculer les gradients en fonction de la loss calculée et les différents paramètres du modèle\n",
        "- On applique les gradients calculés avec l'optimisateur choisi\n",
        "\n",
        "Entraîner le générateur (presque la même chose):\n",
        "- Générer des images fausses à partir de bruit gaussien et en prédire les labels.\n",
        "- Calculer la loss en comparant les prédictions sur les fausses avec des 1 (on veut tromper le discriminateur).\n",
        "- Calculer les gradients en fonction de la loss calculée et les différents paramètres du modèle\n",
        "- On applique les gradients calculés avec l'optimisateur choisi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moZPLFqPSnz9"
      },
      "outputs": [],
      "source": [
        "def train_step(real_images, generator, discriminator, loss, g_opt, d_opt):\n",
        "  global LATENT_DIM\n",
        "  batch_size = real_images.size(dim=0)\n",
        "\n",
        "\n",
        "  noise = torch.randn(size = ...).to(device)\n",
        "  fake_images = ...\n",
        "\n",
        "  ### On entraine le discriminateur\n",
        "\n",
        "  discriminator.zero_grad()\n",
        "\n",
        "  fake_images_predictions = ...\n",
        "  real_images_predictions = ...\n",
        "\n",
        "  fake_labels = ...\n",
        "  fake_labels = fake_labels.type_as(real_images)\n",
        "  real_labels = ...\n",
        "  real_labels = real_labels.type_as(real_images)\n",
        "\n",
        "  ### On calcule la loss pour les vraies et les fausses images\n",
        "\n",
        "  fake_loss = ...\n",
        "  real_loss = ...\n",
        "\n",
        "  disc_loss = (fake_loss + real_loss)/2\n",
        "\n",
        "  ### Backpropagation\n",
        "\n",
        "  disc_loss.backward()\n",
        "  d_opt.step()\n",
        "  #d_opt.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "  ### On entraine le générateur\n",
        "\n",
        "  generator.zero_grad()\n",
        "\n",
        "  fake_images = ...\n",
        "  fake_images_predictions = ...\n",
        "\n",
        "  real_labels = ...\n",
        "  real_labels = real_labels.type_as(real_images)\n",
        "\n",
        "  ### On calcule la loss pour le générateur\n",
        "\n",
        "  gen_loss = ...\n",
        "\n",
        "  ### Backpropagation\n",
        "\n",
        "  gen_loss.backward()\n",
        "  g_opt.step()\n",
        "  #g_opt.zero_grad()\n",
        "  return gen_loss,disc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPUvdewb7Ak5"
      },
      "source": [
        "## Le train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92gcoGyJIQJx"
      },
      "source": [
        "On code la fonction d'entraînement principale.\n",
        "Il reste à compléter la loss et les optimisateurs à utiliser, typiquement ici la `BCELoss()` et `Adam(parameters,lr=2e-4,betas = (0.5, 0.5)` (n'oubliez pas les parenthèses pour la loss !).\n",
        "\n",
        "Dans un premier temps, on peut prendre les mêmes optimiseurs pour les deux quitte à adapter pour tester après (changer le learning rate par exemple pour rééquilibrer un peu l'entraînement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOXC7WKmTI_Z"
      },
      "outputs": [],
      "source": [
        "def train(dataset,generator,discriminator,epochs,fixed_noise):\n",
        "\n",
        "  ###A compléter###\n",
        "  loss = ...\n",
        "  g_opt = ...\n",
        "  d_opt = ...\n",
        "\n",
        "  Lgen_loss = []\n",
        "  Ldisc_loss = []\n",
        "  X = []\n",
        "  j = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    progress_bar = tqdm(dataset)\n",
        "    ##Vu que c'est un dataloader, on ne peut itérer directement dessus avec son indice. On va juste prendre à chaque fois batch par batch.\n",
        "    for _,image_batch in enumerate(progress_bar):\n",
        "        j += 1\n",
        "        real_images = image_batch.to(device)\n",
        "        gen_loss, disc_loss = ...\n",
        "\n",
        "        X.append(j)\n",
        "        Lgen_loss.append(gen_loss.item())\n",
        "        Ldisc_loss.append(disc_loss.item())\n",
        "\n",
        "    clear_output(wait=False)\n",
        "    generate_and_save_plots(X, Lgen_loss, Ldisc_loss) #Définie après, pour générer les courbes des loss\n",
        "    summarize_performance(generator,fixed_noise) #Définie après, pour afficher les images générées"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSDZildZALv"
      },
      "source": [
        "## L'affichage à chaque epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpNuUbbgIQJy"
      },
      "source": [
        "\n",
        "Petite fonction qui affiche les images obtenues à chaque epoch. On va **afficher** 25 images avec la même seed (toujours du même vecteur latent) pour voir l'amélioration progressive de l'image. Ce n'est pas de l'overfitting sur un seul vecteur car on **entraîne** bien à partir de vecteurs différents à chaque fois avant.\n",
        "\n",
        "On affiche aussi après chaque epoch les courbes des loss du générateur et du discriminateur pour suivre l'entraînement et l'arrêter si on constate un souci."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D5j4ZlLXaOh"
      },
      "outputs": [],
      "source": [
        "def summarize_performance(generator,fixed_noise):\n",
        "  fake_images = generator(fixed_noise).detach().cpu()\n",
        "  fake_images = torch.permute(fake_images, (0, 2, 3, 1))\n",
        "  fig = plt.figure(figsize=(12,12))\n",
        "  for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(fake_images[i]*0.5+0.5)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDbwKnC1pLFN"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_plots(X, Lgen_loss, Ldisc_loss):\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    plt.plot(X,Lgen_loss, label = 'gen_loss')\n",
        "    plt.plot(X,Ldisc_loss, label = 'disc_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAx8pu5P7sOR"
      },
      "source": [
        "## Ici vous lancez tout!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNStSQXmIQJ0"
      },
      "source": [
        "Choisissez le nombre d'epochs que vous voulez. Ici une vingtaine d'epochs peut suffire, on va pas attendre 3h quand même."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKElP8cy8dil"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "fixed_noise = torch.randn(size=(25, LATENT_DIM)).to(device)\n",
        "train(x_train,generator,discriminator,EPOCHS,fixed_noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypSz-8HzIQJ0"
      },
      "source": [
        "## Sauvegarde du modèle et inférence\n",
        "\n",
        "Maintenant que votre modèle s'est entraîné pendant 10 minutes voir plus, ce serait con de le perdre et de devoir le réentraîner à chaque fois (en plus ça pollue c'est pas cool).\n",
        "On va donc voir comment le sauvegarder sur votre drive et ensuite l'inférer, c'est-à-dire générer les images sans toucher aux poids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnoImcTtIQJ1"
      },
      "outputs": [],
      "source": [
        "#Mount son drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Google va vous demander alors de vous logger avec votre compte google et accepter les conditions blabla..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECsgo2zaIQJ1"
      },
      "source": [
        "On peut sauvegarder uniquement les poids du modèle à chaque fois pour ne pas avoir à stocker d'autres informations inutiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsFfGkajIQJ1"
      },
      "outputs": [],
      "source": [
        "#On save les poids du générateur dans un fichier .h5 avec la méthode .save_weights\n",
        "torch.save(generator,'/content/drive/My Drive/generator.h5')\n",
        "\n",
        "#Faire pareil pour le discriminateur\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwrekpydIQJ2"
      },
      "source": [
        "Ensuite il ne reste plus qu'à load les poids des deux modèles, générer des vecteurs latents et inférer le modèle dessus.\n",
        "Pour inférer le modèle, on a déjà fait ça dans la boucle d'entraînement, il suffit de le considérer comme une simple fonction et faire ```model(entree)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUrBuwOIIQJ2"
      },
      "outputs": [],
      "source": [
        "#On peut alors charger les poids d'un modèle déjà entraîné avec .load_weights\n",
        "generator = torch.load('/content/drive/My Drive/generator.h5')\n",
        "\n",
        "#Faire pareil pour le discriminateur\n",
        "...\n",
        "\n",
        "#On peut alors générer des images avec le générateur\n",
        "latent_vector = torch.randn(size=(25, LATENT_DIM, 1, 1)).to(device) #Création de 25 vecteurs latents aléatoires pour générer 25 images\n",
        "fake_images = generator(latent_vector).detach().cpu()               #Génération des images avec le générateur\n",
        "fake_images = torch.permute(fake_images, (0, 2, 3, 1))              #On déplace les dimensions de notre tenseur pour les adapter à un format compatible avec plt\n",
        "\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(fake_images[i]*0.5+0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Si vous êtes arrivé jusqu'ici dans le TP, c'est très bien !**\n",
        "\n",
        "**Vous pouvez appeler un membre de l'association pour récupérer votre certification Automatants !**"
      ],
      "metadata": {
        "id": "78zVNMAqArDR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prn-zEwC7jhy"
      },
      "source": [
        "# À développer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq_5UD18IQJ3"
      },
      "source": [
        "## Autres datasets\n",
        "\n",
        "Vous pouvez essayer de générer des images sur d'autres datasets comme Fashion MNIST, CIFAR, CelebA ou n'importe quel type d'images qui vous font plaisir. S'il faut le télécharger et l'importer sur Colab, vous pouvez directement monter votre Drive et uploader votre dataset sur ce dernier.\n",
        "\n",
        "## Autres architectures\n",
        "\n",
        "Vous pouvez rajouter des blocs dans le générateur ou le discriminateur, essayer d'ajouter du Dropout, enlever les biais dans les couches, modifier la dimension de l'espace latent, initialiser les poids d'une certaine façon... Si vous êtes déjà à jour sur les CNN avancés (ResNET, MobileNet, EfficientNet), vous pouvez essayer de faire un GAN sur ces bases, ce qui permettra de résoudre certains problèmes comme les vanishing gradient.\n",
        "\n",
        "## Tenter de forcer des problèmes\n",
        "\n",
        "Faites n'importe quoi !\n",
        "\n",
        "Plus sérieusement, cela peut arriver surtout sur des datasets plus compliqués, la plupart des choses fonctionnent pour MNIST à part si on fait exprès de provoquer des problèmes.\n",
        "\n",
        "Vous pouvez essayer par exemple de déséquilibrer l'entraînement en modifiant les learning rate, en rajoutant beaucoup de couches seulement d'un côté...etc\n",
        "\n",
        "Ici on peut forcer le mode collapse en prenant une dimension latente de seulement 1. Le générateur aura alors plus de risques de générer des images sur un seul mode.\n",
        "\n",
        "## Tips d'entraînement possibles\n",
        "\n",
        "- Utiliser une loss différente, du genre la Wasserstein loss.\n",
        "\n",
        "- Changer le label visé pour les images vraies de 1 en 0.9\n",
        "\n",
        "- Rajouter du bruit sur les images\n",
        "\n",
        "- Ajouter des labels à l'entrée du générateur et du discriminateur (vous pourrez alors même choisir les classes des images générées)\n",
        "\n",
        "- Entraîner le discrminateur plus que le générateur (typiquement entre 3 à 5 boucles d'entraînement à chaque fois que le générateur en fait une).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM6h4yvial7i"
      },
      "source": [
        "# Ce qu'il faut retenir de ce TP\n",
        "- Architecture du générateur et du discriminateur :\n",
        "\n",
        "  `Conv2D(Transpose) -> BatchNorm -> ReLU ou LeakyReLU`\n",
        "\n",
        "  + Spécifités en entrée et sortie selon ce que l'on veut\n",
        "\n",
        "  Eviter les fully connected/denses si possible\n",
        "\n",
        "- Faire une boucle d'entraînement from scratch avec un train_step (très utile pour faire des choses plus compliquées)\n",
        "\n",
        "  `Loss -> gradient_tape.gradient() -> optimizer.apply_gradients`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}